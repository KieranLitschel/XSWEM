{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_xswem.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FiWYLdj1_LE"
      },
      "source": [
        "Make this notebook deterministic."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iK-SetLf1_LF"
      },
      "source": [
        "RANDOM_SEED = 0\n",
        "\n",
        "# Python RNG\n",
        "import random\n",
        "random.seed(RANDOM_SEED)\n",
        "\n",
        "# Numpy RNG\n",
        "import numpy as np\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# TF RNG\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.framework import random_seed\n",
        "random_seed.set_seed(RANDOM_SEED)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZqQuFF4gH_X"
      },
      "source": [
        "Import the necessary modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeltIbDDgHLV",
        "outputId": "6fd60dc3-a6f4-41cb-8338-d5c553a9d2cd"
      },
      "source": [
        "from xswem.model import XSWEM\r\n",
        "!pip install datasets\r\n",
        "from datasets import load_dataset\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "from tensorflow.data import Dataset"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.6/dist-packages (1.1.3)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.6/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.6/dist-packages (from datasets) (2.0.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from datasets) (0.8)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.0.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets) (1.19.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TURK2Q2G1_LF"
      },
      "source": [
        "Load and shuffle the dataset. We keep 10% of the training set for validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKXF40_l1_LG",
        "outputId": "3b756a91-7c39-49c5-9cd7-1f4b72aedc04"
      },
      "source": [
        "ag_news = load_dataset('ag_news')\n",
        "ag_news = ag_news.shuffle({\"train\":RANDOM_SEED,\"test\":RANDOM_SEED})\n",
        "ag_news[\"train\"] = ag_news[\"train\"].train_test_split(test_size=0.1,seed=RANDOM_SEED)\n",
        "ag_news_train, ag_news_valid = ag_news[\"train\"][\"train\"], ag_news[\"train\"][\"test\"]\n",
        "X, y = ag_news_train[\"text\"], ag_news_train[\"label\"]\n",
        "X_valid, y_valid = ag_news_valid[\"text\"], ag_news_valid[\"label\"]\n",
        "ag_news_test = ag_news[\"test\"]\n",
        "X_test, y_test = ag_news_test[\"text\"], ag_news_test[\"label\"]"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using custom data configuration default\n",
            "Reusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/fb5c5e74a110037311ef5e904583ce9f8b9fbc1354290f97b4929f01b3f48b1a)\n",
            "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/ag_news/default/0.0.0/fb5c5e74a110037311ef5e904583ce9f8b9fbc1354290f97b4929f01b3f48b1a/cache-9edf62a6acdef7c2.arrow\n",
            "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/ag_news/default/0.0.0/fb5c5e74a110037311ef5e904583ce9f8b9fbc1354290f97b4929f01b3f48b1a/cache-005431702e078c3e.arrow\n",
            "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/ag_news/default/0.0.0/fb5c5e74a110037311ef5e904583ce9f8b9fbc1354290f97b4929f01b3f48b1a/cache-8fa4e42aef0940f7.arrow and /root/.cache/huggingface/datasets/ag_news/default/0.0.0/fb5c5e74a110037311ef5e904583ce9f8b9fbc1354290f97b4929f01b3f48b1a/cache-0aa5eb9aa593fda3.arrow\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRiR4MAZc8wp"
      },
      "source": [
        "Build the tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZHrhk-9c9PD"
      },
      "source": [
        "NUM_WORDS = 16000\n",
        "tokenizer = Tokenizer(num_words=NUM_WORDS, oov_token=\"UNK\")\n",
        "tokenizer.fit_on_texts(X)\n",
        "vocab_map = {i+1: tokenizer.index_word[i+1] for i in range(NUM_WORDS)}\n",
        "output_map = {0: \"World\", 1: \"Sport\", 2: \"Business\", 3: \"Tech\"}"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSxUALxTzgJl"
      },
      "source": [
        "Build the dataset pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VrddGHtzhTg"
      },
      "source": [
        "BATCH_SIZE = 32\r\n",
        "NUM_LABELS = len(output_map)\r\n",
        "\r\n",
        "train_dataset = Dataset.from_tensor_slices((X,y))\r\n",
        "valid_dataset = Dataset.from_tensor_slices((X_valid,y_valid))\r\n",
        "test_dataset = Dataset.from_tensor_slices((X_test,y_test))\r\n",
        "\r\n",
        "# shuffle the train datasets\r\n",
        "train_dataset = train_dataset.shuffle(BATCH_SIZE*2)\r\n",
        "\r\n",
        "# tokenize the text and one hot encode the labels\r\n",
        "# we only keep unique tokens as XSWEM is invariant to token frequency and order\r\n",
        "tokenize = lambda text, label: (tf.py_function(lambda text: np.unique(tokenizer.texts_to_sequences([str(text.numpy())])[0]), inp=[text], Tout=tf.int32), tf.one_hot(label,NUM_LABELS))\r\n",
        "train_dataset = train_dataset.map(tokenize,num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n",
        "valid_dataset = valid_dataset.map(tokenize,num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n",
        "test_dataset = test_dataset.map(tokenize,num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n",
        "\r\n",
        "# pre-fetch so that GPU spends less time waiting\r\n",
        "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\r\n",
        "valid_dataset = valid_dataset.prefetch(tf.data.experimental.AUTOTUNE)\r\n",
        "test_dataset = test_dataset.prefetch(tf.data.experimental.AUTOTUNE)\r\n",
        "\r\n",
        "# padded batch allows us to handle varying sentence lengths\r\n",
        "train_dataset = train_dataset.padded_batch(BATCH_SIZE,padded_shapes=([None],[NUM_LABELS]))\r\n",
        "valid_dataset = valid_dataset.padded_batch(BATCH_SIZE,padded_shapes=([None],[NUM_LABELS]))\r\n",
        "test_dataset = test_dataset.padded_batch(BATCH_SIZE,padded_shapes=([None],[NUM_LABELS]))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zp_q4YY-1_LJ"
      },
      "source": [
        "Build XSWEM model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHirkOJk1_LK"
      },
      "source": [
        "model = XSWEM(128, \"softmax\", vocab_map, output_map, mask_zero=True, dropout_rate=0.5)\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=1)\n",
        "model.compile(optimizer, loss=\"categorical_crossentropy\", metrics=\"accuracy\")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJzGyYIw1_LL"
      },
      "source": [
        "Train XSWEM model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPDuVH7E1_LM",
        "outputId": "e83ca8ff-34a3-4e38-91d7-d13cd9e96112"
      },
      "source": [
        "model.fit(train_dataset, validation_data=valid_dataset, epochs=5)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "3375/3375 [==============================] - 64s 19ms/step - loss: 0.6194 - accuracy: 0.7692 - val_loss: 0.2947 - val_accuracy: 0.9060\n",
            "Epoch 2/5\n",
            "3375/3375 [==============================] - 62s 18ms/step - loss: 0.3482 - accuracy: 0.8842 - val_loss: 0.2677 - val_accuracy: 0.9112\n",
            "Epoch 3/5\n",
            "3375/3375 [==============================] - 62s 18ms/step - loss: 0.3021 - accuracy: 0.8982 - val_loss: 0.2629 - val_accuracy: 0.9143\n",
            "Epoch 4/5\n",
            "3375/3375 [==============================] - 63s 19ms/step - loss: 0.2718 - accuracy: 0.9089 - val_loss: 0.2689 - val_accuracy: 0.9159\n",
            "Epoch 5/5\n",
            "3375/3375 [==============================] - 63s 19ms/step - loss: 0.2514 - accuracy: 0.9158 - val_loss: 0.2665 - val_accuracy: 0.9150\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3e8ee42be0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6LVE4TdeTuN"
      },
      "source": [
        "Test XSWEM model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlsFjVmdCzW1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68719f4f-9920-4a87-e683-8d49e544ba7d"
      },
      "source": [
        "model.evaluate(test_dataset)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "238/238 [==============================] - 4s 16ms/step - loss: 0.2813 - accuracy: 0.9116\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.2812957763671875, 0.9115789532661438]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    }
  ]
}