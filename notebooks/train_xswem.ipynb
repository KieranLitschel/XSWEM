{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "train_xswem.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "You can run this notebook in Google Colab by clicking the badge below.\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/KieranLitschel/XSWEM/blob/main/notebooks/train_xswem.ipynb)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Download the XSWEM code from GitHub."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!git clone https://github.com/KieranLitschel/XSWEM\n",
    "!mv XSWEM/* ./\n",
    "!rm -rf XSWEM"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5FiWYLdj1_LE"
   },
   "source": [
    "Make this notebook deterministic."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iK-SetLf1_LF"
   },
   "source": [
    "RANDOM_SEED = 0\n",
    "\n",
    "# Python RNG\n",
    "import random\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# Numpy RNG\n",
    "import numpy as np\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# TF RNG\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import random_seed\n",
    "random_seed.set_seed(RANDOM_SEED)"
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZqQuFF4gH_X"
   },
   "source": [
    "Import the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FeltIbDDgHLV",
    "outputId": "b99519bb-e051-4550-de91-7f9046810f02"
   },
   "source": [
    "!pip install -r requirements.txt\n",
    "from xswem.model import XSWEM\n",
    "from datasets import load_dataset\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.data import Dataset"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.6/dist-packages (1.1.3)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets) (0.3.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.6/dist-packages (from datasets) (2.0.0)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from datasets) (0.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets) (1.1.5)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets) (0.70.11.1)\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets) (1.19.4)\n",
      "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.6/dist-packages (from datasets) (4.41.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.23.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2.8.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TURK2Q2G1_LF"
   },
   "source": [
    "Load and shuffle the dataset. We keep 10% of the training set for validation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WKXF40_l1_LG",
    "outputId": "a103903d-48a9-4025-dfd2-8d7b840fd888"
   },
   "source": [
    "ag_news = load_dataset('ag_news')\n",
    "ag_news = ag_news.shuffle({\"train\":RANDOM_SEED,\"test\":RANDOM_SEED})\n",
    "ag_news[\"train\"] = ag_news[\"train\"].train_test_split(test_size=0.1,seed=RANDOM_SEED)\n",
    "ag_news_train, ag_news_valid = ag_news[\"train\"][\"train\"], ag_news[\"train\"][\"test\"]\n",
    "X, y = ag_news_train[\"text\"], ag_news_train[\"label\"]\n",
    "X_valid, y_valid = ag_news_valid[\"text\"], ag_news_valid[\"label\"]\n",
    "ag_news_test = ag_news[\"test\"]\n",
    "X_test, y_test = ag_news_test[\"text\"], ag_news_test[\"label\"]"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset ag_news (/root/.cache/huggingface/datasets/ag_news/default/0.0.0/fb5c5e74a110037311ef5e904583ce9f8b9fbc1354290f97b4929f01b3f48b1a)\n",
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/ag_news/default/0.0.0/fb5c5e74a110037311ef5e904583ce9f8b9fbc1354290f97b4929f01b3f48b1a/cache-be508598455d4f77.arrow\n",
      "Loading cached shuffled indices for dataset at /root/.cache/huggingface/datasets/ag_news/default/0.0.0/fb5c5e74a110037311ef5e904583ce9f8b9fbc1354290f97b4929f01b3f48b1a/cache-20770064a94e03e1.arrow\n",
      "Loading cached split indices for dataset at /root/.cache/huggingface/datasets/ag_news/default/0.0.0/fb5c5e74a110037311ef5e904583ce9f8b9fbc1354290f97b4929f01b3f48b1a/cache-02c5ea6d2a89dadf.arrow and /root/.cache/huggingface/datasets/ag_news/default/0.0.0/fb5c5e74a110037311ef5e904583ce9f8b9fbc1354290f97b4929f01b3f48b1a/cache-49ffeef1df99ba93.arrow\n"
     ],
     "name": "stderr"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lRiR4MAZc8wp"
   },
   "source": [
    "Build the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6ZHrhk-9c9PD"
   },
   "source": [
    "NUM_WORDS = 16000\n",
    "FILTERS = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n0123456789'\n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS, oov_token=\"UNK\", filters=FILTERS)\n",
    "tokenizer.fit_on_texts(X)\n",
    "vocab_map = {i+1: tokenizer.index_word[i+1] for i in range(NUM_WORDS)}\n",
    "output_map = {0: \"World\", 1: \"Sport\", 2: \"Business\", 3: \"Tech\"}"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zSxUALxTzgJl"
   },
   "source": [
    "Build the dataset pipeline."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8VrddGHtzhTg"
   },
   "source": [
    "BATCH_SIZE = 32\r\n",
    "NUM_LABELS = len(output_map)\r\n",
    "\r\n",
    "train_dataset = Dataset.from_tensor_slices((X,y))\r\n",
    "valid_dataset = Dataset.from_tensor_slices((X_valid,y_valid))\r\n",
    "test_dataset = Dataset.from_tensor_slices((X_test,y_test))\r\n",
    "\r\n",
    "# shuffle the train datasets\r\n",
    "train_dataset = train_dataset.shuffle(BATCH_SIZE*2)\r\n",
    "\r\n",
    "# tokenize the text and one hot encode the labels\r\n",
    "# we only keep unique tokens as XSWEM is invariant to token frequency and order\r\n",
    "tokenize = lambda text, label: (tf.py_function(lambda text: np.unique(tokenizer.texts_to_sequences([str(text.numpy())])[0]), inp=[text], Tout=tf.int32), tf.one_hot(label,NUM_LABELS))\r\n",
    "train_dataset = train_dataset.map(tokenize,num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n",
    "valid_dataset = valid_dataset.map(tokenize,num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n",
    "test_dataset = test_dataset.map(tokenize,num_parallel_calls=tf.data.experimental.AUTOTUNE)\r\n",
    "\r\n",
    "# pre-fetch so that GPU spends less time waiting\r\n",
    "train_dataset = train_dataset.prefetch(tf.data.experimental.AUTOTUNE)\r\n",
    "valid_dataset = valid_dataset.prefetch(tf.data.experimental.AUTOTUNE)\r\n",
    "test_dataset = test_dataset.prefetch(tf.data.experimental.AUTOTUNE)\r\n",
    "\r\n",
    "# padded batch allows us to handle varying sentence lengths\r\n",
    "train_dataset = train_dataset.padded_batch(BATCH_SIZE,padded_shapes=([None],[NUM_LABELS]))\r\n",
    "valid_dataset = valid_dataset.padded_batch(BATCH_SIZE,padded_shapes=([None],[NUM_LABELS]))\r\n",
    "test_dataset = test_dataset.padded_batch(BATCH_SIZE,padded_shapes=([None],[NUM_LABELS]))"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zp_q4YY-1_LJ"
   },
   "source": [
    "Build XSWEM model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZHirkOJk1_LK"
   },
   "source": [
    "model = XSWEM(128, \"softmax\", vocab_map, output_map, mask_zero=True, dropout_rate=0.5)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=5e-1)\n",
    "model.compile(optimizer, loss=\"categorical_crossentropy\", metrics=\"accuracy\")"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJzGyYIw1_LL"
   },
   "source": [
    "Train XSWEM model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bPDuVH7E1_LM",
    "outputId": "de0c7c6b-bdfe-4253-9c33-d6b6ea12086f"
   },
   "source": [
    "model.fit(train_dataset, validation_data=valid_dataset, epochs=3)"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "3375/3375 [==============================] - 64s 19ms/step - loss: 0.5898 - accuracy: 0.7845 - val_loss: 0.3155 - val_accuracy: 0.9029\n",
      "Epoch 2/3\n",
      "3375/3375 [==============================] - 63s 19ms/step - loss: 0.3009 - accuracy: 0.8990 - val_loss: 0.2856 - val_accuracy: 0.9105\n",
      "Epoch 3/3\n",
      "3375/3375 [==============================] - 62s 19ms/step - loss: 0.2558 - accuracy: 0.9159 - val_loss: 0.2786 - val_accuracy: 0.9119\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f39dee42c50>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 7
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z6LVE4TdeTuN"
   },
   "source": [
    "Test XSWEM model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hlsFjVmdCzW1",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "07b4bebd-cbbd-4b2c-bfd8-d3e83258e71a"
   },
   "source": [
    "model.evaluate(test_dataset)"
   ],
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "238/238 [==============================] - 4s 16ms/step - loss: 0.2896 - accuracy: 0.9105\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[0.28958284854888916, 0.9105263352394104]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 8
    }
   ]
  }
 ]
}